:_module-type: REFERENCE
//Specify the module-type as either "CONCEPT, PROCEDURE, or REFERENCE"

// Module included in the following assemblies:
//
// * assemblies/planning-limits.adoc

[id="cluster-maximums-environment_{context}"]
= OpenShift Container Platform testing environment and configuration

[role="_abstract"]
The following table lists the OpenShift Container Platform environment and configuration on which the cluster maximums are tested for the AWS cloud platform.

[options="header",cols="8*"]
|===
| Node |Flavor |vCPU |RAM(GiB) |Disk type|Disk size(GiB)/IOS |Count |Region

|Control Plane/etcd ^[1]^
|m5.2xlarge
|8
|32
|io1
|350 / 1000
|3
|us-west-2

|Infrastructure ^[2]^
|m5.xlarge
|4
|16
|gp2
|300 / 900
|3
|us-west-2

|Workload ^[3]^
|m5.2xlarge
|8
|32
|gp2
|300 / 900
|3
|us-west-2

|Worker
|m5.2xlarge
|8
|32
|gp2
|300 / 900
|3
|us-west-2
|===
[.small]
--
1. io1 disks are used for control plan/etcd nodes because etcd is I/O intensive and latency sensitive. A greater number of IOPS can be required, depending on usage.
2. Infrastructure nodes are used to host monitoring components because Prometheus can claim a large amount of memory, depending on usage patterns.
3. Workload nodes are dedicated to run performance and scalability workload generators.
--

Larger cluster sizes and higher object counts may be reachable. However, the sizing of the infrastructure nodes limits the amount of memory that is available to Prometheus. When creating, modifying, or deleting objects, Prometheus stores the metrics in its memory for roughly 3 hours prior to flushing the metrics to disk. If the rate of creation, modification, or deletion of objects is too high, Prometheus can easily be overwhelmed and fail due to lack of memory resources.
